{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from landcover_classification import prepare_training_data, load_training_data, classification, plot_classified_image\n",
    "from wcps_rasdaman import wcps_rasdaman\n",
    "from geo_utils import stack_geotiff\n",
    "import datetime\n",
    "from osgeo import gdal\n",
    "from owslib.wcs import WebCoverageService\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dataset\n",
    "As data input for this demo we will use inSAR coherence data from a 5-D data cube we imported in Rasdaman for the SInCohMap project. Specifically for the landcover classification demo only the 36 images with the shortest baseline will be used (the images along the red line in the below coherence matrix).\n",
    "\n",
    "The training data is derived from a mixture of the LandcoverInformationSystem (LISS) from South Tyrol, Open Street Map data to refine the training data in urban areas and the High Resolution Copernicus Forest layer to further refine the forest class. It is stored as shapefile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"coherence_matrix_shortest_baseline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below image you can see the 5-D datacube in Rasdamans webinterface datacube\n",
    "<img src=\"sincohmap_cube.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sincohmap data cube has two time axis (master & slave date), something owslib (we usally use to derive metadata information from Rasdaman via WCS) canÂ´t work with. \n",
    "Therefore the information about the available timesteps must be gatherd manually from the DescribeCoverage response XML using the lxml.etree module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timepositions_sincohmap(wcs, coverage_name):\n",
    "    from owslib.etree import etree\n",
    "    from owslib.util import datetime_from_iso\n",
    "    def nsWCS2(tag):\n",
    "        return '{http://www.opengis.net/wcs/2.0}' + tag\n",
    "    def gmlgrid(tag):\n",
    "        return '{http://www.opengis.net/gml/3.3/rgrid}' + tag\n",
    "\n",
    "    tree = wcs.getDescribeCoverage(coverage_name)\n",
    "\n",
    "    # Find the Grid Element where all axis information is stored\n",
    "    gridelem = tree.find(nsWCS2('CoverageDescription/') + '{http://www.opengis.net/gml/3.2}domainSet/' + gmlgrid(\n",
    "        'ReferenceableGridByVectors'))\n",
    "\n",
    "    all_axes = gridelem.findall(gmlgrid('generalGridAxis/') + gmlgrid('GeneralGridAxis'))\n",
    "\n",
    "    # First axis is master, second is slave, save as datetime objects to list timepostions_master/slave\n",
    "\n",
    "    master_date = all_axes[0].find(gmlgrid('coefficients')).text.split(' ')\n",
    "    timepositions_master = []\n",
    "    for x in master_date:\n",
    "        x = x.replace('\"', '')\n",
    "        t_date = datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        timepositions_master.append(t_date)\n",
    "\n",
    "    slave_date = all_axes[1].find(gmlgrid('coefficients')).text.split(' ')\n",
    "    timepositions_slave = []\n",
    "    for x in slave_date:\n",
    "        x = x.replace('\"', '')\n",
    "        t_date = datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        timepositions_slave.append(t_date)\n",
    "    return timepositions_master, timepositions_slave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Define inputs #\n",
    "#################\n",
    "prepare_train = 0\n",
    "\n",
    "#      COH  days  coh_VV coh_VH   6d   12d     18d    24d     36d    48d    60d    72d    84d    96d   INT   int_VV  int_VH    \n",
    "inp = [True, 6,   True,   True,  True, False, False, False,  False, False, False, False, False, False, False, False, False]\n",
    "\n",
    "#Define timespan for Coherence\n",
    "COH      = inp[0]\n",
    "TIMESPAN = inp[1]\n",
    "coh_VV   = inp[2]\n",
    "coh_VH   = inp[3]\n",
    "_6days   = inp[4]\n",
    "_12days  = inp[5]\n",
    "_18days  = inp[6]\n",
    "_24days  = inp[7]\n",
    "_36days  = inp[8]\n",
    "_48days  = inp[9]\n",
    "_60days  = inp[10]\n",
    "_72days  = inp[11]\n",
    "_84days  = inp[12]\n",
    "_96days  = inp[13]\n",
    "\n",
    "# Define bands to use in intensity\n",
    "INT      = inp[14]\n",
    "int_VV   = inp[15]\n",
    "int_VH   = inp[16]\n",
    "\n",
    "# Select WCS server and service version\n",
    "\n",
    "wcs = WebCoverageService('http://saocompute.eurac.edu/sincohmap/rasdaman/ows?', version='2.0.1')\n",
    "coh_coverage = 'SOUTH_TYROL_4x19_UTM'\n",
    "#int_coverage = 'SOUTH_TYROL_WEST_ASC_GEO_GAMMA_1x5_UTM_RESAMPLE'\n",
    "\n",
    "#Define year\n",
    "YEAR = 2017\n",
    "\n",
    "# Define bbox + coordinate system (epsg:32632)\n",
    "ulx = 651990\n",
    "uly = 5181010\n",
    "lrx = 671990\n",
    "lry = 5161010\n",
    "bbox_epsg32632 = 'E('+str(ulx)+':'+str(lrx)+'), N('+str(lry)+':'+str(uly)+'),' #more digits -> N (y) , E (x)\n",
    "\n",
    "# Build filename from input\n",
    "filename = ''\n",
    "if (coh_coverage[:5]=='SOUTH'): filename = 'ST_'\n",
    "filename += str(YEAR) + '_'\n",
    "if COH:\n",
    "    filename+='coh_'\n",
    "    if coh_VV: filename += 'VV_'\n",
    "    if coh_VH: filename += 'VH_'\n",
    "    if _6days: filename += '6_'\n",
    "    if _12days: filename += '12_'\n",
    "    if _18days: filename += '18_'\n",
    "    if _24days: filename += '24_'\n",
    "    if _36days: filename += '36_'\n",
    "    if _48days: filename += '48_'\n",
    "    if _60days: filename += '60_'\n",
    "    if _72days: filename += '72_'\n",
    "    if _84days: filename += '84_'\n",
    "    if _96days: filename += '96_'\n",
    "if INT:\n",
    "    filename += 'int_'\n",
    "    if int_VV: filename += 'VV'\n",
    "    if int_VH: filename += 'VH'\n",
    "\n",
    "\n",
    "filename += '.tif'\n",
    "print('[*] FILENAME: ',filename)\n",
    "\n",
    "# Define the output filename of the data stack\n",
    "stack_name = './Output_Data/ST/Sources/' + filename\n",
    "timepositions_master, timepositions_slave = extract_timepositions_sincohmap(wcs, coh_coverage)\n",
    "\n",
    "\n",
    "############################################\n",
    "# Prepare the training/validation datasets #\n",
    "############################################\n",
    "\n",
    "# Define the number of sampling points & classification level (1 OR 3)\n",
    "nr_points = 10000\n",
    "class_level = 'LEVEL_3'\n",
    "sampling_strategy = 'equal'\n",
    "\n",
    "# Path to training data\n",
    "training_path = './Training_Data/ST/ST_LVL3_bordermask.tif'\n",
    "#vector_path = './Training_Data/ST/SInCohMap_STyrol_Validation_RR_WGS84_H32.shp'\n",
    "vector_path = ''\n",
    "# Mask for steep terrain\n",
    "mask_path = './Training_Data/ST/geo_RR_combined_mask_radar_inv.tif'\n",
    "# path of where to store the extracted training samples\n",
    "sample_path = './Training_Data/ST/ST_training_10000_samples_equal.p'\n",
    "\n",
    "# define the path to where to store your resulting classified map\n",
    "class_path = './Output_Data/ST/Classified/classified_' + filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the timepostions, we can acutally send WCPS requests to the Rasdaman server. This is done with the* wcps_rasdaman* module, which sends the request to the server and converts the response in a way that we can work with it directly in Python.\n",
    "In this example we are downloading TIFFs of the single images (VH band) and stack it to a multiband TIFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Download raster data subset via WCPS & stack it #\n",
    "###################################################\n",
    "print(\"TIMESPAN = \", TIMESPAN)\n",
    "files = []\n",
    "index = 0\n",
    "for i in range(len(timepositions_master)):\n",
    "    if (timepositions_master[i].year == YEAR):\n",
    "        if index == 0:\n",
    "            master_reference = timepositions_master[i]\n",
    "            print(\"REF: \", master_reference)\n",
    "            index = 1\n",
    "        timestep_master = timepositions_master[i].isoformat()\n",
    "        print(\"MASTER: \", timestep_master)\n",
    "        if COH:\n",
    "            for j in range(len(timepositions_slave)):\n",
    "                if (timepositions_slave[j].year == YEAR) and \\\n",
    "                   (timepositions_slave[j].timetuple().tm_yday > timepositions_master[i].timetuple().tm_yday + 0) and \\\n",
    "                   (timepositions_slave[j].timetuple().tm_yday <= timepositions_master[i].timetuple().tm_yday + TIMESPAN):              \n",
    "                    timestep_slave = timepositions_slave[j].isoformat()\n",
    "                    print(\"SLAVE: \",timepositions_slave[j].isoformat())\n",
    "                    if(i > 0):\n",
    "                        if ((timepositions_master[i].timetuple().tm_yday-master_reference.timetuple().tm_yday) % TIMESPAN != 0):\n",
    "                            continue\n",
    "                    if not _6days:\n",
    "                        if (timepositions_slave[j].timetuple().tm_yday - timepositions_master[i].timetuple().tm_yday) == 6:\n",
    "                            continue\n",
    "                    if not _12days:\n",
    "                        if (timepositions_slave[j].timetuple().tm_yday - timepositions_master[i].timetuple().tm_yday) == 12:\n",
    "                            continue\n",
    "                    if not _18days:\n",
    "                        if (timepositions_slave[j].timetuple().tm_yday - timepositions_master[i].timetuple().tm_yday) == 18:\n",
    "                            continue\n",
    "                    if not _24days:\n",
    "                        if (timepositions_slave[j].timetuple().tm_yday - timepositions_master[i].timetuple().tm_yday) == 24:\n",
    "                            continue\n",
    "                    print(\"SLAVE MASTER DIFF: \",timepositions_slave[j].timetuple().tm_yday - timepositions_master[i].timetuple().tm_yday)\n",
    "                    if coh_VV:\n",
    "                        query_coh_VV = ('for c in (' + coh_coverage + ') return encode' + \n",
    "                                 '(((float) c.Coherence_VV[' + bbox_epsg32632 +\n",
    "                                     'master_date(\"' + timestep_master + '\"), ' + \n",
    "                                     'slave_date(\"' + timestep_slave + '\")' + \n",
    "                                 ']),\"tiff\")')\n",
    "                        subset_coherence_VV = wcps_rasdaman(query_coh_VV, ip = 'saocompute.eurac.edu/sincohmap')\n",
    "                        files.append(subset_coherence_VV)\n",
    "                    if coh_VH:\n",
    "                        query_coh_VH = ('for c in (' + coh_coverage + ') return encode' + \n",
    "                                 '(((float) c.Coherence_VH[' + bbox_epsg32632 +\n",
    "                                     'master_date(\"' + timestep_master + '\"), ' + \n",
    "                                     'slave_date(\"' + timestep_slave + '\")' + \n",
    "                                 ']),\"tiff\")')\n",
    "                        subset_coherence_VH = wcps_rasdaman(query_coh_VH, ip = 'saocompute.eurac.edu/sincohmap')\n",
    "                        files.append(subset_coherence_VH)\n",
    "        if INT:\n",
    "            if int_VV:\n",
    "                query_int_VV = ('for c in (' + int_coverage + ') return encode' + \n",
    "                     '(((float) 10*log(c.Intensity_VV[' + bbox_epsg32632 +\n",
    "                         'date(\"' + timestep_master + '\") ' + \n",
    "                     '])),\"tiff\")')\n",
    "                subset_int_VV = wcps_rasdaman(query_int_VV, ip = 'saocompute.eurac.edu/sincohmap')\n",
    "                files.append(subset_int_VV)\n",
    "            if int_VH:\n",
    "                query_int_VH = ('for c in (' + int_coverage + ') return encode' + \n",
    "                         '(((float) 10*log(c.Intensity_VH[' + bbox_epsg32632 +\n",
    "                             'date(\"' + timestep_master + '\") ' + \n",
    "                         '])),\"tiff\")')\n",
    "                subset_int_VH = wcps_rasdaman(query_int_VH, ip = 'saocompute.eurac.edu/sincohmap')\n",
    "                files.append(subset_int_VH)\n",
    "\n",
    "stack_geotiff(files, outtif=stack_name) #the raster image to be classififed with scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landcover classification for CORINE Level 3 with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################\n",
    "# # Print data stack information #\n",
    "# ################################\n",
    "\n",
    "raster_dataset = gdal.Open(stack_name, gdal.GA_ReadOnly)\n",
    "geo_transform = raster_dataset.GetGeoTransform()\n",
    "print(geo_transform)\n",
    "extent = []\n",
    "extent.append(geo_transform[0])\n",
    "extent.append(geo_transform[0] + geo_transform[1]*raster_dataset.RasterXSize)\n",
    "extent.append(geo_transform[3] + geo_transform[5]*raster_dataset.RasterYSize)\n",
    "extent.append(geo_transform[3])\n",
    "print(extent)\n",
    "print(\"X: \", raster_dataset.RasterXSize)\n",
    "print(\"Y: \", raster_dataset.RasterYSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Create new training samples #\n",
    "###############################\n",
    "\n",
    "#training_samples, training_labels, bands_data, projection, geo_transform,training_pixels = prepare_training_data(training_path, stack_name, class_level, nr_points, sampling_strategy, mask_path=mask_path, sample_path=sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# Load saved training samples #\n",
    "###############################\n",
    "\n",
    "# this step is necessary if you want to use the same training pixels locations generated previously and stored\n",
    "training_samples, training_labels, bands_data, projection, geo_transform, training_pixels = load_training_data(stack_name, training_path, sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Random forest classification #\n",
    "################################\n",
    "\n",
    "test_labels = [None]\n",
    "test_samples = [None]\n",
    "classified_image, classifier = classification(training_samples, training_labels, test_labels, test_samples, bands_data, projection, geo_transform,'rf', gridsearch=False, mask_path =mask_path, n_estimators = 1000, class_path=class_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Enjoy the looks of your MAP #\n",
    "###############################\n",
    "\n",
    "# plot your image to see the resulting classified map\n",
    "%matplotlib inline\n",
    "plot_classified_image(class_path=class_path, plot_map_info=True, plot_map_legend=True, plot_title=class_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:forest] *",
   "language": "python",
   "name": "conda-env-forest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
