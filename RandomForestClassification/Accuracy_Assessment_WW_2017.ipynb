{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Setup your environment for classification #\n",
    "#############################################\n",
    "\n",
    "# import necessary libraries\n",
    "from Death_to_Kappa import *\n",
    "from osgeo import gdal, ogr\n",
    "from geo_utils import create_raster_from_vector\n",
    "import glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Quality Assessment for your Land Cover Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Setup the labels of your classes and their IDs #\n",
    "##################################################\n",
    "\n",
    "# in this case we use corine land cover nomenclature with a 3 digit class code\n",
    "\n",
    "label_con = [[-1, 0, 100, 110, 111, 112, 120, 121, 122, 123, 124, 130, 131, 132, 133, 140, 141, 142, 200, 210, 211, 212, 213, 220, 221, 222, 223, 230, 231, 240, 241, 242, 243, 244, 300, 310, 311, 312, 313, 320, 321, 322, 323, 324, 330, 331, 332, 333, 334, 335, 400, 410, 411, 412, 420, 421, 422, 423, 500, 510, 511, 512, 520, 521, 522, 523],\n",
    "            ['Error', 'Zero Class', 'Artificial', 'Urban fabric', 'Cont. urban fabric', 'Disc. urban fabric', 'Industrial/Commercial/Transport units', 'Industrial/commercial units', 'Road/rail networks, associated land', 'Port areas', 'Airport', 'Mine/dump/construction sites', 'Mineral extraction sites', 'Dump sites', 'Construction sites', 'Artificial/non-agricultural vegetated areas', 'Green urban areas', 'Sport/Leisure facilities', 'Agricultural', 'Arable land', 'Non-irrigated arable land', 'Permanently irrigated land', 'Rice fields', 'Permanent crops', 'Vineyards', 'Fruit trees/berry plantations', 'olive groves', 'Pastures', 'Pastures', 'Heterogenous agricultural areas', 'Annual crops/Permanent crops', 'Complex cultivation patterns', 'agricultur/significant areas of natural veg.', 'Agro-forestry areas', 'Forest/semi natural', 'Forest', 'Broad-leaved forest', 'Coniferous forest', 'Mixed forest', 'Scrub/herbaceous veg.', 'Natural grasslands', 'Moors/heathland', 'Sclerophyllous veg.', 'Transitional woodland-shrub', 'Open spaces w/ little veg.', 'Beaches/Dunes/Sands', 'Bare rocks', 'Sparsely vegetated areas', 'Burnt areas', 'Glaciers and perpetual snow', 'Wetlands', 'Inland wetlands', 'Inland marshes', 'Peat bogs', 'Maritime wetlands', 'Salt marshes', 'Salines', 'Intertidal flats', 'Inland waters', 'Water courses', 'Water bodies', 'Marine waters', 'Coastal lagoons', 'Estuaries', 'Sea/ocean']]\n",
    "label_l=len(label_con[0])\n",
    "for i_label in range(0,label_l-1):\n",
    "    print(label_con[0][i_label], ':', label_con[1][i_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get path to classified images\n",
    "classified = []\n",
    "for file in sorted(glob.glob('./Output_Data/WW/Classified/classified*.tif')):\n",
    "    classified.append(file)\n",
    "# list files that have been found\n",
    "classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract titles from file names\n",
    "titles = []\n",
    "for i in classified:\n",
    "    split1 = i.split('/')\n",
    "    split2 = split1[-1].split('.')\n",
    "    titles.append(split2[0])\n",
    "# list titles that have been found\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add path to reference data\n",
    "test_paths = dict((\n",
    "                ('Wielkopolska', ('./Training_Data/WW/WW_LVL3.tif')),\n",
    "                ))\n",
    "ds_st = gdal.Open('./Training_Data/WW/WW_LVL3.tif')\n",
    "test_st = ds_st.ReadAsArray().flatten()\n",
    "test_labels = dict((\n",
    "                ('Wielkopolska', (test_st)),\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Generate confusion matrices and create heat maps #\n",
    "####################################################\n",
    "\n",
    "aggregate = False\n",
    "\n",
    "for path in classified:\n",
    "    ds = gdal.Open(path)\n",
    "    predicted = ds.ReadAsArray().flatten()\n",
    "    path_to_reference = ''\n",
    "    test = []\n",
    "    coverage_name = 'Wielkopolska'\n",
    "    test = test_labels[coverage_name]\n",
    "\n",
    "    #choose your Corine Level of Classification\n",
    "    level = 'LEVEL_3'\n",
    "#     if 'lvl_3' in path:\n",
    "#         level = 'LEVEL_3'\n",
    "#     else:\n",
    "#         level = 'LEVEL_1'\n",
    "            \n",
    "    if aggregate:\n",
    "        test[(test > 220) & (test < 230)] = 220\n",
    "        test[(test == 321)] = 333\n",
    "        test[(test == 324)] = 310\n",
    "\n",
    "        predicted[(predicted > 220) & (predicted < 230)] = 220\n",
    "        predicted[(predicted == 321)] = 333\n",
    "        predicted[(predicted == 324)] = 310\n",
    "    \n",
    "    label_code = np.union1d(test, predicted)\n",
    "    \n",
    "    a =[]\n",
    "    for i in label_code:\n",
    "        a.append(np.where(label_con[0]==i))\n",
    "        #print(i,':',len(np.where(predicted==i)[0]), ' ', len(np.where(test==i)[0]))\n",
    "        \n",
    "    b = np.unique(a)\n",
    "    label = []\n",
    "    for i in range(0, len(b)):\n",
    "        label.append(label_con[1][b[i]])\n",
    "    \n",
    "    split1 = path.split('/')\n",
    "    split2 = split1[-1].split('.')\n",
    "    title = split2[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))   \n",
    "    mat = metrics.confusion_matrix(test, predicted)\n",
    "    \n",
    "    norm_mat = mat.astype('float') / mat.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    mat = sns.heatmap(norm_mat.T, square=True, annot=True, fmt='.2f', cbar=False, xticklabels=label, yticklabels=label)\n",
    "    \n",
    "    plt.title('Confusion Matrix', fontsize=25)\n",
    "    \n",
    "    plt.xlabel('true label', fontsize=18)\n",
    "    plt.ylabel('predicted label', fontsize=18)\n",
    "    title2 = title.strip('classified_WW_')\n",
    "    fig.savefig('./Output_Data/WW/ConfusionMatrixImages/WW_CMHM_' + title2 + '.jpeg', bbox_inches=\"tight\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Generate confusion matrices and create ascii tables #\n",
    "#######################################################\n",
    "aggregate = False\n",
    "\n",
    "for path in classified:\n",
    "    ds = gdal.Open(path)\n",
    "    predicted = ds.ReadAsArray().flatten()\n",
    "    \n",
    "    coverage_name = 'Wielkopolska'\n",
    "\n",
    "    #predicted = predicted[mData>0]    # Mask application\n",
    "    test = test_labels[coverage_name]\n",
    "    #test = test[mData>0]\n",
    "\n",
    "    if 0 in np.unique(test):            # Do not consider zero class\n",
    "        test_nonzero = np.nonzero(test)\n",
    "        test = test[test_nonzero]\n",
    "        predicted = predicted[test_nonzero]\n",
    "\n",
    "        path_to_reference = test_paths[coverage_name]\n",
    "    \n",
    "    #choose your Corine Level of Classification\n",
    "    #if 'LVL3' in path:\n",
    "    level = 'LEVEL_3'\n",
    "    #else:\n",
    "    #    level = 'LEVEL_1'\n",
    "            \n",
    "    if aggregate:\n",
    "        test[(test > 220) & (test < 230)] = 220\n",
    "        test[(test == 321)] = 333\n",
    "        test[(test == 324)] = 310\n",
    "\n",
    "        predicted[(predicted > 220) & (predicted < 230)] = 220\n",
    "        predicted[(predicted == 321)] = 333\n",
    "        predicted[(predicted == 324)] = 310\n",
    "    \n",
    "    label_code = np.union1d(test, predicted)\n",
    "    a =[]\n",
    "    for i in label_code:\n",
    "        a.append(np.where(label_con[0]==i))\n",
    "    b = np.unique(a)\n",
    "    label = []\n",
    "    for i in range(0, len(b)):\n",
    "        label.append(label_con[1][b[i]])\n",
    "        \n",
    "    row_label = copy.deepcopy(label)\n",
    "    column_label = copy.deepcopy(label)\n",
    "    row_label.append('column total')\n",
    "    row_label.append('producer\\'s accuracy')\n",
    "    column_label.append('row total')\n",
    "    column_label.append('user\\'s accuracy')\n",
    "    print(label)\n",
    "    split1 = path.split('/')\n",
    "    split2 = split1[-1].split('.')\n",
    "    title = split2[0]\n",
    "    cm = ctab(predicted, test)\n",
    "\n",
    "    size = cm.shape[0]\n",
    "    cm_tot = np.zeros((size+2,size+2),dtype=np.float32)\n",
    "    cm_tot[:size,:size] = cm\n",
    "    cm_tot[size,:size] = cm.sum(axis=0)\n",
    "    cm_tot[:size,size] = cm.sum(axis=1)\n",
    "    # compute user's and producer's accuracy\n",
    "    oa = 0\n",
    "    for i in range(size): \n",
    "        oa += cm_tot[i,i]\n",
    "        # user's accuracy\n",
    "        cm_tot[i,size+1] = cm_tot[i,i] / cm_tot[i,size]\n",
    "        # producer's accuracy\n",
    "        cm_tot[size+1,i] = cm_tot[i,i] / cm_tot[size,i]\n",
    "    cm_tot[size+1,size+1] = oa / cm.sum()\n",
    "    \n",
    "    title2 = title.strip('classified_WW_')\n",
    "    cmdf = pd.DataFrame(cm_tot, index = row_label, columns = column_label)\n",
    "    pd.DataFrame.to_csv(cmdf, path_or_buf = './Output_Data/WW/ConfusionMatrixCSV/WW_CM_' + title2 + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Generate report for each maps that has been assessed #\n",
    "########################################################\n",
    "\n",
    "aggregate = False\n",
    "\n",
    "for path in classified:\n",
    "    \n",
    "    #get name of participant\n",
    "    a = path.split('/')\n",
    "    a2 = a[-1].split('.')\n",
    "    title = a2[0]\n",
    "    mask_array = []\n",
    "    #############################################################\n",
    "    # generate reference image from Training Data\n",
    "    coverage_name = 'Wielkopolska'\n",
    "    aggregate = False\n",
    "        \n",
    "    path_to_reference = test_paths[coverage_name]\n",
    "\n",
    "    level = 'LEVEL_3'\n",
    "        \n",
    "    df = kstat(path, path_to_reference, perCategory = False, aggregate = aggregate)\n",
    "    print(title,': ',df)\n",
    "    title2 = title.strip('classified_WW_')\n",
    "    pd.DataFrame.to_csv(df, path_or_buf = './Output_Data/WW/Stats/WW_QR_' + title2 + '.csv')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:forest] *",
   "language": "python",
   "name": "conda-env-forest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
